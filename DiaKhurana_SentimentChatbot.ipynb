{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch gradio --quiet\n"
      ],
      "metadata": {
        "id": "W2VQLxBhHGPu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import gradio as gr\n"
      ],
      "metadata": {
        "id": "RfgLxaHiHHIr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using DistilBERT fine-tuned on SST-2 (binary sentiment)\n",
        "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n"
      ],
      "metadata": {
        "id": "lu0atB5nHKb0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment(text):\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "    # Model output\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    # Apply softmax to convert logits â†’ probabilities\n",
        "    scores = F.softmax(outputs.logits, dim=1)\n",
        "\n",
        "    # Extract positive/negative scores\n",
        "    positive = float(scores[0][1])\n",
        "    negative = float(scores[0][0])\n",
        "\n",
        "    # Compound score: simple (pos - neg)\n",
        "    compound = positive - negative\n",
        "\n",
        "    # Label assignment\n",
        "    label = \"Positive\" if compound >= 0 else \"Negative\"\n",
        "\n",
        "    return label, compound\n"
      ],
      "metadata": {
        "id": "IUNBzzRVHLJY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(sentiment_label):\n",
        "    if sentiment_label == \"Positive\":\n",
        "        return \"Nice!I'm glad to hear that. How can I assist you next?\"\n",
        "    else:\n",
        "        return \"I'm sorry you're feeling this way.Want to talk about it?\""
      ],
      "metadata": {
        "id": "j18b4eukHNYs"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EXIT_WORDS = [\"bye\", \"goodbye\", \"see you\", \"stop\", \"exit\"]\n",
        "GREETINGS = [\"hey\", \"hi\", \"hello\", \"good morning\", \"good evening\", \"good afternoon\"]\n",
        "\n",
        "def chatbot_controller(user_text):\n",
        "    text = user_text.lower().strip()\n",
        "\n",
        "    # Exit logic\n",
        "    if any(word == text for word in EXIT_WORDS):\n",
        "        return \"Session ended. Thank you.\"\n",
        "\n",
        "    # Greeting logic\n",
        "    if any(word == text for word in GREETINGS):\n",
        "        return \"Hello. How can I help you today?\"\n",
        "\n",
        "    # Sentiment flow\n",
        "    label, compound = analyze_sentiment(user_text)\n",
        "    bot_reply = generate_response(label)\n",
        "\n",
        "    return f\"{bot_reply}\\n\\n(Sentiment: {label}, score={compound:.3f})\"\n"
      ],
      "metadata": {
        "id": "yNnuHVpFHPIl"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_chat(user_message, history):\n",
        "    response = chatbot_controller(user_message)\n",
        "    history.append((user_message, response))\n",
        "    return history, \"\"\n"
      ],
      "metadata": {
        "id": "_PDrOAUOHRgw"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(theme=\"soft\") as demo:\n",
        "\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <h1 style=\"text-align:center; color:#4A90E2;\"> Transformer Sentiment Chatbot</h1>\n",
        "        <p style=\"text-align:center; font-size:16px;\">\n",
        "            Talk to the chatbot below. Say <b>bye</b> to end the conversation.\n",
        "        </p>\n",
        "        <hr>\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chatbot_ui = gr.Chatbot(\n",
        "        height=350,\n",
        "        label=\"Chat Window\",\n",
        "        bubble_full_width=False,\n",
        "    )\n",
        "\n",
        "    user_input = gr.Textbox(\n",
        "        label=\"Type your message...\",\n",
        "        placeholder=\"Write something here...\",\n",
        "    )\n",
        "\n",
        "    send_btn = gr.Button(\"Send\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "    def gradio_chat(user_message, history):\n",
        "        reply = chatbot_controller(user_message)\n",
        "\n",
        "        # Add proper conversational format\n",
        "        history.append(\n",
        "            (f\" {user_message}\", f\" {reply}\")\n",
        "        )\n",
        "        return history, \"\"\n",
        "\n",
        "    send_btn.click(\n",
        "        gradio_chat,\n",
        "        inputs=[user_input, chatbot_ui],\n",
        "        outputs=[chatbot_ui, user_input]\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va2tu1GWHTMS",
        "outputId": "d7dce9cb-fbe5-4e37-d1bb-ad90200f7d23"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1689523438.py:1: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=\"soft\") as demo:\n",
            "/tmp/ipython-input-1689523438.py:13: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot_ui = gr.Chatbot(\n",
            "/tmp/ipython-input-1689523438.py:13: DeprecationWarning: The 'bubble_full_width' parameter will be removed in Gradio 6.0. This parameter no longer has any effect.\n",
            "  chatbot_ui = gr.Chatbot(\n",
            "/tmp/ipython-input-1689523438.py:13: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot_ui = gr.Chatbot(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "UE7i4I3fHU1H",
        "outputId": "b50bc3db-a139-41b8-e561-4ac7d9f854e5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
            "----\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ec17cbc1fa7279b351.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ec17cbc1fa7279b351.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ]
}